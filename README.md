
# MXINT-Based Exponent-Sign Approximation for Self-Attention Pruning in Vision and Diffusion Transformers

## Overview
Attention pruning on transformer-based models
- vision transformer: DeiT-tiny, DeiT-small, DeiT-base
- diffusion transformer: DiT-XL/2 (256x256), PixArt- $\alpha$ (256x256)
1. MXINT Quantization on transformer-based models
2. Attention pruning
   - Top-k based attention pruning
3. Approximated attention pruning
   - Proposed: MXINT8 Exponent-Sign approximated Q, K

##
