
# MXINT-Based Exponent-Sign Approximation for Self-Attention Pruning on Vision and Diffusion Transformers

1. MXINT Quantization on transformer-based models
2. Attention pruning
   - Top-k based attention pruning
3. Approximated attention pruning
   - Proposed: MXINT8 Exponent-Sign approximated Q, K
