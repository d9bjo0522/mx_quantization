
# MXINT-Based Exponent-Sign Approximation for Self-Attention Pruning in Vision and Diffusion Transformers

## Overview
### Attention pruning on transformer-based models
- Vision transformer: DeiT-tiny, DeiT-small, DeiT-base
- Diffusion transformer: DiT-XL/2 (256x256), PixArt- $\alpha$ (256x256)

### Attention pruning
- Top-k based attention pruning
- Approximated attention pruning
   - approximated Q, K for lightweight Q*K calculation for top-k selection


##
